dev_dir: ${oc.env:DEV_PATH}

hydra:
  run:
    dir: ${dev_dir}/im2flow2act/experiment/flow_generation/animateFlow/Preprocess
  
entity_name: Preprocess
project_name: AnimateFlow
debug: false

training:
  num_train_epochs: 5000
  log_frequency: 10
  max_train_steps: null
  gradient_accumulation_steps: 1
  gradient_checkpointing: False
  lr_scheduler: "constant"
  lr_warmup_steps: 0
  mixed_precision: "fp16"
  enable_xformers_memory_efficient_attention: True
  # dataloader 
  batch_size: 4
  num_workers: 4
  shuffle: True
  pin_memory: True
  persistent_workers: True
  drop_last: False
  # ckpt
  ckpt_frequency: 50
  # load_pretrain_weight
  load_pretrain_weight: True

animateflow_kwargs:
  clip_model: "openai/clip-vit-large-patch14"
  global_image_size: [224,224]
  freeze_visual_encoder: True
  global_condition_type: "all"
  emb_dim: 768

load_pretrain_motion_module: False
lora:
  rank: 128


pretrained_model_path: ${oc.env:DEV_PATH}/im2flow2act/pretrain_weights/StableDiffusion1.5

cfg_random_null_text: True
cfg_random_null_text_ratio: 0.1
max_grad_norm: 1.0

noise_scheduler_kwargs:
  num_train_timesteps: 1000
  beta_start:          0.00085
  beta_end:            0.012
  beta_schedule:       "linear"
  steps_offset:        1
  clip_sample:         false


unet_additional_kwargs:
  use_motion_module              : true
  motion_module_resolutions      : [ 1,2,4,8 ]
  unet_use_cross_frame_attention : false
  unet_use_temporal_attention    : false

  # in_channels: 16 

  motion_module_type: Vanilla
  motion_module_kwargs:
    num_attention_heads                : 8
    num_transformer_block              : 1
    attention_block_types              : [ "Temporal_Self", "Temporal_Self" ]
    temporal_position_encoding         : true
    temporal_position_encoding_max_len : 48
    temporal_attention_dim_div         : 1
    zero_initialize                    : true


optimizer:
  learning_rate: 1e-4
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_weight_decay: 1e-2
  adam_epsilon: 1e-08

dataset:
  _target_: im2flow2act.flow_generation.dataloader.animateflow_dataset.AnimateFlowDataset_Libero
  data_pathes: [
    "None"
  ]
  grid_size: 16
  frame_sampling_method: "uniform"
  frame_resize_shape: [224,224]
  point_tracking_img_size: [256,256]
  diff_flow: False
  n_sample_frames: 25
  # max_episode: null
  preprocess: True
  max_episode: null
  proc_cache: <YOUR OUTPUT PATH HERE>
  # max_episode: [4,4,4,4,4]
  proc_dataset_name: 'bridge' 
  dataset_name: 'bridge'


